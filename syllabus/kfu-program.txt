Понимать место анализа данных в ландшафте науки и сферы информационных технологий.
Знать возможности специализированного программного обеспечения для анализа данных.
Знать основные методы машинного обучения, их возможности и ограничения.
Знать теоретические основы методов машинного обучения.
Уметь выбирать подходящий метод машинного обучения для решения задач анализа данных.
Уметь применять специализированное программное обеспечения для решения задач анализа данных.


Введение в анализ данных
Линейная регрессия
Стохастический градиентный спуск
Логистическая регрессия
Метод опорных векторов


Цели и задачи анализа данных. Связь анализа данных с математикой, статистикой, компьютерными науками. Статистика, машинное обучение, искусственный интеллект, прогностическая аналитика, "наука о данных" (Data Science), большие данные" (Big Data): сходства и различия. Место анализа данных в сфере информационных технологий. Программный инструментарий анализа данных: R, scikit-learn, Spark, TensorFlow, Vowpal Wabbit. Языки программирования, используемые для анализа данных. Проблема обработки больших объемов данных и быстродействия систем работы с большими данными. Источники данных для анализа. Публичные наборы данных. Сбор открытых данных в сети Интернет. Визуализация данных. Возможности библиотек matplotlib, seaborn и bokeh.

Задача регрессии. Простая линейная регрессия. Функция потерь. Графическое представление линейной регрессии. Задача линейной регрессии как задача оптимизации. Метод наименьших квадратов. Многомерная линейная регрессия. Проблема коллинеарности и способы ее преодоления. Проблема переобучения регрессионной модели и способы ее преодоления. Регуляризация. Гребневая регрессия. Метод лассо. Эластичная сеть. Выбор параметров для гребневой регрессии, лассо-регрессии и эластичной сети. Алгоритмическая сложность регрессии. Робастная регрессия: регрессия Тейла-Сена, метод RANSAC. Линейная регрессия в пакете R. Линейная регрессия в библиотеке scikit-learn. Линейная регрессия в системе Spark.

Задача оптимизации без ограничений. Градиент функции. Метод градиентного спуска. Графическое представление метода градиентного спуска. Случай недифференцируемости целевой функции, субградиент. Сходимость метода градиентного спуска. Обучение модели машинного обучения как задача минимизации функции потерь. Недостатки классического метода градиентного спуска в условиях больших наборов данных и потоков данных. Метод стохастического градиентного спуска. Метод стохастического градиентного спуска для мини-выборок. Графическое представление метода стохастического градиентного спуска. Сходимость метода градиентного спуска. Выбор скорости обучения. Модификации метода стохастического градиентного спуска: метод момента, усредненный стохастический градиентный спуск, метод адаптивного градиента, алгоритм адаптивной оценки момента. Обучение модели линейной регрессии методом стохастического градиентного спуска. Стохастический градиентный спуск в пакете R, библиотеке scikit-learn. Линейная регрессия в системе Spark. Линейная регрессия в Vowpal Wabbit. Метод стохастического градиентного спуска для линейной регрессии в библиотеке TensorFlow.

Задача бинарной классификации. Модель логистической регрессии. Правдоподобие. Отношение правдоподобия. Логистическая функция. Логистическая функция потерь. Логистическая регрессия как задача минимизации логистической функции потерь. Проблема нахождения точного решения задачи минимизации логистической функции потерь. Приближенное решение задачи оптимизации. Метод стохастического градиентного спуска для логистической регрессии. Многоклассовая логистическая регрессия. Логистическая регрессия в пакете R. Логистическая регрессия в бибилиотеке scikit-learn. Логистическая регрессия в системе Spark. Логистическая регрессия в Vowpal Wabbit. Метод стохастического градиентного спуска для логистической регрессии в библиотеке TensorFlow.

Формулировка метода опорных векторов. Графическая интерпретация метода опорных векторов. Разделяющая гиперплоскость. Случай линейной разделимости. Метод опорных векторов как задача оптимизации с ограничением. Случай линейной неразделимости. Переменные нежесткости. Сведение к задаче оптимизации без ограничений. Петлевая функция потерь. Метод опорных векторов как задача минимизации петлевой функции потерь. Двойственная задача оптимизации. Нахождение решения прямой задачи путем решения двойственной. Метод ядра. Полиномиальное ядро. Неявное преобразование пространства признаков полиномиальным ядром. Гауссовское ядро. Неявное преобразование пространства признаков гауссовским ядром. Ограничения на применение метода ядра в случае больших объемов данных и потоков данных.  Метод стохастического градиентного спуска для метода опорных векторов. Метод опорных векторов в пакете R. Методов опорных векторов в библиотеке scikit-learn. Метод опорных векторов в Spark.



Контрольная работа 1
1. Реализуйте метод стохастического градиентного спуска для простой линейной регрессии с постоянной скоростью обучения.
2. Реализуйте метод стохастического градиентного спуска для простой линейной регрессии, используя метод адаптивного градиента.
3. Составьте программу, визуализирующую простую линейную регрессию и позволяющую вычислять функцию потерь для заданного набора данных в зависимости от значений коэффициентов регрессии, задаваемых пользователем.
4. По заданному набору данных составьте линейную регрессию, используя библиотеку scikit-learn. Визуализируйте полученный результат с помощью библиотеки matplotlib.
5. Реализуйте обучение модели линейной регрессии с помощью стохастического градиентного спуска, по подходящему потоку данных с веб-сайта www.quandl.com. Визуализируйте процесс обучения.

Контрольная работа 2
1. Реализуйте метод стохастического градиентного спуска для логистической регрессии с постоянной скоростью обучения.
2. Реализуйте метод стохастического градиентного спуска для метода опорных векторов, используя метод адаптивного градиента.
3. По заданному набору данных составьте логистическую регрессию, используя библиотеку TensorFlow. Визуализируйте полученный результат с помощью библиотеки Bokeh.
4. Обучите модель логистической регрессии и модель метода опорных векторов с гауссовским ядром на одном и том же наборе данных. Сравните полученные прогностические модели.
5. Для заданного набора данных визуализируйте преобразование пространства признаков полиномиальным ядром степени 2.


Вопросы к зачету
1. Источники данных. Сбор данных в сети Интернет.
2. Инструментарий и возможности визуализации данных.
3. Простая линейная регрессия. Реализация простой линейной регрессии в программном средстве по выбору.
4. Многомерная линейная регрессия. Реализация многомерной линейной регрессии в программном средстве по выбору.
5. Линейная регрессия как задача оптимизации. Подходы к решению задачи.
6. Коллинеарность в линейной регрессии. Переобучение линейной регрессии.
7. Гребневая регрессия. Реализация гребневой регрессии в программном средстве по выбору.
8. Лассо-регрессия. Реализация лассо-регрессии в программном средстве по выбору.
9. Эластичная сеть. Реализация эластичной сети в программном средстве по выбору.
10. Регрессия Тейла-Сена. Метод RANSAC.
11. Алгоритмическая сложность регрессии.
12. Метод градиентного спуска (базовый).
13. Метод градиентного спуска для недифференцируемой целевой функции.
14. Сходимость метода градиентного спуска.
15. Метод стохастического градиентного спуска (базовый).
16. Реализация метода стохастического градиентного спуска в программном средстве по выбору.
17. Модификация метода стохастического градиентного спуска: метод момента.
18. Модификация метода стохастического градиентного спуска: усредненный стохастический градиентный спуск.
19. Модификация метода стохастического градиентного спуска: метод адаптивного градиента.
20. Модификация метода стохастического градиентного спуска: алгоритм адаптивной оценки момента.
21. Стохастический градиентный спуск для линейной регрессии.
22. Линейная регрессия как искусственная нейронная сеть.
23. Модель логистической регрессии. Реализация логистической регрессии в программном средстве по выбору.
24. Функция потерь для логистической регрессии: мотивация и свойства.
25. Логистическая регрессия как искусственная нейронная сеть.
26. Метод опорных векторов. Графическая интерпретация метода опорных векторов. Математическая формулировка метода опорных векторов.
27. Метод опорных векторов. Случай линейной неразделимости.
28. Метод опорных векторов. Прямая и двойственная задача оптимизации.
29. Метод опорных векторов. Метод ядра. Полиномиальное ядро. Неявное преобразование пространства признаков полиномиальным ядром.
30. Гауссовское ядро. Неявное преобразование пространства признаков гауссовским ядром.
31. Метод стохастического градиентного спуска для метода опорных векторов. Ограничения метода.


Методические рокоммендации

На лекционных занятиях следует вести запись лекции в тетради, даже если преподаватель предоставит Вам доступ к лекционному материалу в электронном формате. Запись вручную необходима для лучшего освоения материала и лучшей ориентации в материале. Вопросы во время лекции приветствуются; чтобы задать вопрос, необходимо поднять руку и дождаться, когда преподаватель даст Вам слово. На практических занятиях следует выполнять задания, данные преподавателем, самостоятельно, а в случае возникновения сложностей консультироваться с преподавателем. Приветствуются групповые обсуждения и разбор сложных вопросов студентами на доске. При выполнении практических заданий не злоупотребляйте готовыми решениями, которые можно найти в сети Интернет: сначала следует попытаться решить задачу самостоятельно, а лишь потом можно сравнить свое решение с решениями, найденными в Интернет. При подготовке к контрольной работе 1 следует обращать внимание на математические аспекты изучаемого материала, поскольку именно они представляют наибольшую сложность. При неуверенном владении математическим аппаратом первоначально следует повторить материал, касающийся дифференцируемости и выпуклости функций одной и многих переменных, экстремальных задач, уравнений прямой на плоскости и в пространстве. Также следует определиться заранее, на каком языке программирования Вам будет наиболее комфортно выполнять задания, связанные с программированием. В заданиях, связанных с программированием, не допускается использование специализированных библиотек, если не оговорено обратное. При подготовке к контрольной работе 2 следует убедиться в свободном владении математическим материалом, связанным с разложением функции в ряд Тейлора, вероятностями событий и случайных величин, расстоянием между точкой и прямой, аффинными преобразованиями и кривыми в многомерном пространстве, и при необходимости восполнить пробелы в знаниях. Как показывает практика, отдельную трудность может представлять одновременное использование сведений из нескольких ранее изучавшихся дисциплин, чем изобилует данный курс; следует быть к этому готовым. При подготовке к зачету следует в первую очередь сосредоточиться на понимании общего подхода, который лежит за изучаемым материалом, а также логики, согласно которой можно перейти от общей канвы к частным случаям. Во время зачета будет оцениваться в первую очередь Ваше понимание общих идей, понимание взаимосвязей между разными изучаемыми методами и умение выводить конкретные результаты из более общих идей и методов. В связи с этим следует остерегаться линейного заучивания материала, которое будет весьма трудоемко и при этом всё равно не позволит подготовиться на высокий балл. На вопросы зачета следует давать развернутый ответ, избегайте односложных ответов -- таковые, как правило, свидетельствуют о недостаточной подготовке студента. При нечеткой формулировке вопроса следует рассмотреть возможные интерпретации смысла вопроса и изложить свои соображение преподавателю. 

   

___________________________________________
Знать возможности и сферы применения анализа данных. 
Знать теоретические основы методов анализа данных. 
Уметь реализовывать изучаемые методы на алгоритмическом языке программирования. 
Уметь обрабатывать реальные данные с помощью изучаемых методов. 
Понимать сложности аналитической обработки больших массивов данных и знать подходы к их преодолению. 
Уметь ориентироваться в профессиональной литературе по анализу данных и машинному обучению. 
Уметь представлять результаты анализа данных в надлежащем виде. 


Кластеризация
Понижение размерности
Рекомендательные системы
Анализ графов социальных сетей


Точки, пространства и расстояния. Стратегии кластеризации. "Проклятие размерности". Иерархическая кластеризация. Иерархическая кластеризация в евклидовом пространстве. Эффективность иерархической кластеризации. Альтернативные правила управления иерархической кластеризацией. Иерархическая кластеризация в неевклидовых пространствах. Алгоритм k средних. Основы алгоритма k средних. Инициализация кластеров в алгоритме k средних. Выбор правильного значения k. Алгоритм Брэдли-Файяда-Рейна (BFR). Обработка данных в алгоритме BFR. Алгоритм CURE. Этап инициализации в CURE. Завершение работы алгоритма CURE. Кластеризация в неевклидовых пространствах. Представление кластеров в алгоритме GRGPF. Инициализация дерева кластеров. Добавление точек в алгоритме GRGPF. Разделение и объединение кластеров. Кластеризация для потоков и параллелизм. Модель потоковых вычислений. Алгоритм кластеризации потока. Инициализация интервалов. Объединение кластеров. Кластеризация в параллельной среде.

Собственные значения и собственные векторы. Вычисление собственных значений и собственных векторов. Нахождение собственных пары степенным методом. Матрица собственных векторов. Метод главных компонент. Использование собственных векторов для понижения размерности. Матрица расстояний. Сингулярное разложение. Определение сингулярного разложения. Интерпретация сингулярного разложения. Понижение размерности с помощью сингулярного разложения. Запросы с использованием концептов. Вычисление сингулярного разложения матрицы. CUR-декомпозиция. Определение CUR-декомпозиции. Правильный выбор строк и столбцов. Построение средней матрицы. Полная CUR-декомпозиция. Исключение дубликатов строк и столбцов.

Модель рекомендательной системы. Матрица предпочтений. Длинный хвост. Применения рекомендательных систем. Заполнение матрицы предпочтений. Рекомендации на основе фильтрации содержимого. Профили объектов. Выявление признаков документа. Получение признаков объектов из меток. Представление профиля объекта. Профили пользователей. Рекомендование объектов пользователям на основе содержимого. Алгоритм классификации. Коллаборативная фильтрация. Измерение сходства. Двойственность сходства. Кластеризация пользователей и объектов. Понижение размерности. UV-декомпозиция. Среднеквадратичная ошибка. Инкрементное вычисление UV-декомпозиции. Оптимизация произвольного элемента. Построение полного алгоритма UV-декомпозиции. Задача NetFlix.

Социальные сети. Социальные сети как графы. Разновидности социальных сетей. Графы с вершинами нескольких типов. Кластеризация графа социальной сети. Метрики для графов социальных сетей. Применение стандартных методов кластеризации. Промежуточность. Алгоритм Гирвана-Ньюмана. Использование промежуточности для нахождения сообществ. Прямое нахождение сообществ. Нахождение клик. Полные двудольные графы. Нахождение полных двудольных подграфов. Существование полных двудольных графов. Разрезание графов. Нормализованные разрезы. Некоторые матрицы, описывающие графы. Собственные значения матрицы Лапласа. Нахождение пересекающихся сообществ. Природа сообществ. Оценка максимального правдоподобия. Модель графа принадлежности. Случайные блуждания в социальном графе. Случайное блуждание с перезапуском. Подсчет треугольников. Алгоритм нахождения треугольников. Оптимальность алгоритма нахождения треугольников. Нахождение треугольников с помощью MapReduce. Окрестности в графах. Ориентированные графы и окрестности. Диаметр графа. Транзитивное замыкание и достижимость. Вычисление транзитивного замыкания с помощью MapReduce. Интеллектуальное транзитивное замыкание. Транзитивное замыкание посредством сокращения графа. Аппроксимация размеров окрестностей.



1. Выполните иерархическую кластеризацию одномерного множества точек 1, 4, 9, 16, 25, 36, 49, 64, 81 при условии, что кластеры представляются их центроидой (средним), и на каждом шаге кластеры с наиболее близкими центроидами сливаются. 
2. Рассмотрим пространство строк с редакционным расстоянием в качестве меры расстояния. Приведите пример набора строк, таких что если выбрат кластроиду минимизацией сумм расстояний до других точек, то в качестве кластроиды получаем одну точку, а если выбрать кластроиду минимизацией максимального расстояния до других точек, то кластроидой становится другая точка. 
3. Выполните алгоритм BDMO с p = 3 на следующих 1-мерных евклидовых данных: 1, 45, 80, 24, 56, 71, 17, 40, 66, 32, 48, 96, 9, 41, 75, 11, 58, 93, 28, 39, 77. В качестве алгоритма кластеризации возьмите алгоритм k-средних с k=3. 
4. Дана матрица M. 
(a) Чему равно M^T M и M M^T? 
(b) Вычислите собственные векторы и собственные значения для M^T M. 
(c) Какими должны быть собственные значения M M^T? 
(d) Найдите собственные векторы M M^T, используя собственные значения из части (c). 
5. Дано сингулярное разложение матрицы. Найдите псевдообратную матрицу Мура-Пенроуза данной матрицы. 



1. Алгоритм интеллектуального транзитивного замыкания разбивает пути произвольной длины на "голову" и "хвост" определенной длины. Каковы длины "головы" и "хвоста" для путей длины 7, 8 и 9? 
2. Три компьютера A, B и C имеют следующие числовые признаки: 
Пр|Процес|Диск|Память 
A | 3.06 | 500| 6 
B | 2.68 | 320| 4 
C | 2.92 | 640| 6 
Можно представить эти значения как определяющие вектор для каждого компьютера. Можно вычислить косинусное расстояние между любыми из двух векторов, но если не масштабировать элементы, то будет доминировать размер диска, а остальные компоненты будут практически невидимы. Пусть 1 -- масштабный множитель для скорость процессора, alpha -- для объема диска, beta -- для объема оперативной памяти. 
(a) В терминах alpha и beta вычислите косинусы углов между векторами для каждой пары из трех компьютеров. 
(b) Каковы углы между векторами, если alpha = beta = 1? 
(c) Каковы углы между векторами, если alpha = 0.01 и beta = 0.5? 
(d) Один из способов выбрать масштабные множители -- сделать каждый обратно пропорциональным среднему значению его компонент. Каковы будут значения alpha и beta и каков будет угол между векторами? 
3. Три компьютера A, B и C имеют следующие числовые признаки: 
Пр|Процес|Диск|Память 
A | 3.06 | 500| 6 
B | 2.68 | 320| 4 
C | 2.92 | 640| 6 
Некоторый пользователь дал следующие рейтинги трем перечисленным компьютерам: A: 4 звезды, B: 2 звезды, C: 5 звезд. 
(a) Нормализуйте рейтинги пользователя. 
(b) Вычислите профиль пользователя с компонентами для скорости процессора, объема диска и объема оперативной памяти. 
4. Дано сообщество, состоящее из 2n узлов. Разделите это сообщество на две группы по n членов случайным образом и сформируйте двудольный граф между двумя группами. Пусть средняя степень узлов двудольного графа равна d. Найдите множество максимальных пар (t, s), где t <= s, такое, что
экземпляр K_{s,t} обязательно существует, для следующих пар n и d:
(a) n = 20, d = 5.
(b) n = 200, d = 150.
(c) n = 1000, d = 400.
Под словом "максимальная" подразумевается, что не существует другой пары (s', t'), такой, что выполняется и s' >= s, и
t' >= t.
5. По данному графу составьте:
(a) Матрицу смежностей.
(b) Матрицу степеней.
(c) Матрицу Лапласа.


Вопросы к зачету: 
1. Кластеризация. Стратегии кластеризации. 
2. Кластеризация. "Проклятие размерности". 
3. Иерархическая кластеризация. 
4. Эффективность иерархической кластеризации. 
5. Алгоритм k средних. 
6. Алгоритм Брэдли-Файяда-Рейна. 
7. Алгоритм CURE. 
8. Кластеризация в неевклидовых пространствах. 
9. Представление кластеров в алгоритме GRGPF. 
10. Кластеризация для потоков и параллелизм. Модель потоковых вычислений. 
11. Алгоритм кластеризации потока. 
12. Кластеризация в параллельной среде. 
13. Собственные значения и собственные векторы. Нахождение собственных пары степенным методом. 
14. Метод главных компонент. Использование собственных векторов для понижения размерности. 
15. Сингулярное разложение. Вычисление сингулярного разложения матрицы. 
16. Понижение размерности с помощью сингулярного разложения. 
17. CUR-декомпозиция. 
18. Модель рекомендательной системы. 
19. Применения рекомендательных систем. 
20. Рекомендации на основе фильтрации содержимого. 
21. Коллаборативная фильтрация. 
22. Кластеризация пользователей и объектов. 
23. UV-декомпозиция. 
24. Социальные сети как графы. Разновидности социальных сетей. 
25. Метрики для графов социальных сетей. Применение стандартных методов кластеризации для графа социальной сети. 
26. Алгоритм Гирвана-Ньюмана. 
27. Прямое нахождение сообществ. Нахождение клик. 
28. Нахождение пересекающихся сообществ. 
29. Случайные блуждания в социальном графе. Случайное блуждание с перезапуском. 
30. Окрестности в графах. Ориентированные графы и окрестности. Диаметр графа. 
31. Транзитивное замыкание и достижимость. Интеллектуальное транзитивное замыкание. 
32. Транзитивное замыкание посредством сокращения графа. Аппроксимация размеров окрестностей. 


Методические рокоммендации

На лекционных занятиях следует вести запись лекции в тетради, даже если преподаватель предоставит Вам доступ к лекционному материалу в электронном формате. Запись вручную необходима для лучшего освоения материала и лучшей ориентации в материале. Вопросы во время лекции приветствуются; чтобы задать вопрос, необходимо поднять руку и дождаться, когда преподаватель даст Вам слово. На практических занятиях следует выполнять задания, данные преподавателем, самостоятельно, а в случае возникновения сложностей консультироваться с преподавателем. Приветствуются групповые обсуждения и разбор сложных вопросов студентами на доске. При выполнении практических заданий не злоупотребляйте готовыми решениями, которые можно найти в сети Интернет, сначала следует решить задачу самостоятельно, а затем можно сравнивать с другими решениями. При подготовке к контрольной работе следует обращать внимание на математические аспекты изучаемого материала, поскольку именно они представляют наибольшую сложность. При неуверенном владении математическим аппаратом первоначально следует повторить материал из линейной алгебры и геометрии и дискретной математики, а именно, разделы, связанные с матрицами, собственными значениями и рангом матриц, расстояниями в евклидовом пространстве и теорией графов. Перед контрольной следует определиться заранее, на каком языке программирования Вам будет наиболее комфортно выполнять задания контрольной работы, связанные с программированием. При выполнении заданий контрольной работы, связанных с составлением программы, рекомендуется сначала выполнить первые несколько шагов программы вручную и лишь затем составлять собственно программу. В заданиях, связанных с программированием, не допускается использование специализированных библиотек, если не оговорено обратное. Если Вам не хватает времени на выполние задания контрольной работы в полном объеме, то следует вкратце описать дальнейшую последовательность действий, которые необходимо сделать для завершения задания; таким образом Вы сможете получить частичные баллы. При этом следует отметить, что нехватка времени на выполнение задания, скорее всего, свидетельствует о том, что Вы выполняете задание неоптимальным способом. При подготовке к зачету следует сосредоточиться на выстраивании общей канвы, взаимосвязей и аналогий между различными частями изучаемого материала. Во время зачета будет оцениваться в первую очередь Ваше понимание общих идей, понимание взаимосвязей между разными изучаемыми методами и умение выводить конкретные результаты из более общих идей и методов. В связи с этим следует остерегаться линейного заучивания материала, которое будет весьма трудоемко и при этом всё равно не позволит подготовиться на высокий балл. При подготовке к зачету используйте, но не злоупотребляйте профессиональными форумами и блогами: подобные ресурсы зачастую позволяют найти ответ на конкретный вопрос, однако это зачастую затеняет общие подходы и не способствует формированию знаний в системе. На вопросы зачета следует давать развернутый ответ, избегайте односложных ответов -- таковые, как правило, свидетельствуют о недостаточной подготовке студента. При нечеткой формулировке вопроса следует рассмотреть возможные интерпретации смысла вопроса и изложить свои соображение преподавателю. 




___________________________________________


Машинное обучение на больших данных
Анализ потоков данных
Частые предметные наборы


Модель машинного обучения. Обучающие наборы. Подходы к машинному обучению. Архитектура машинного обучения. Перцептроны. Обучение перцептрона с нулевым порогом. Сходимость перцептронов. Алгоритм Winnow. Переменный порог. Многоклассовые перцептроны. Преобразование обучающего набора. Проблемы, связанные с перцептронами. Параллельная реализация перцептронов. Метод опорных векторов. Механизм метода опорных векторов. Нормировка гиперплоскости. Нахождение оптимальных приближенных разделителей. Нахождение решений в методе опорных векторов с помощью градиентного спуска. Стохастический градиентный спуск. Параллельная реализация метода опорных векторов. Обучение по ближайшим соседям. Инфраструктура для вычисления ближайших соседей. Обучение по одному ближайшему соседу. Обучение одномерных функций. Ядерная регрессия. Данные в многомерном евклидовом пространстве. Неевклидовы метрики. Сравнение методов обучения. 

Потоковая модель данных. Система управления потоками данных. Примеры источников потоков данных. Запросы к потокам. Проблемы обработки потоков. Выборка данных из потока. Получение репрезентативной выборки. Динамическое изменение размера выборки. Фильтрация потоков. Фильтр Блума. Подсчет различных элементов в потоке. Проблема Count-Distinct. Алгоритм Флажоле-Мартена. Комбинирование оценок. Требования к памяти. Оценивание моментов. Определение моментов. Алгоритм Алона-Матиаса-Сегеди для вторых моментов. Моменты высших порядков. Обработка бесконечных потоков. Подсчет единиц в окне. Стоимость точного подсчета. Алгоритм Датара-Гиониса-Индыка-Мотвани (DGIM). Требования к объему памяти для алгоритма DGIM. Ответы на вопросы в алгоритме DGIM. Поддержание условий DGIM. Уменьшение погрешности. Обобщения алгоритма подсчета единиц. Затухающие окна. Задача о самых частых элементах. Определение затухающего окна. Нахождение самых популярных элементов.

Модель корзины покупок. Определение частого предметного набора. Применения частых предметных наборов. Ассоциативные правила. Поиск ассоциативных правил с высокой достоверностью. Корзины покупок и алгоритм Apriori. Представление данных о корзинах покупок. Использование оперативной памяти для подсчета предметных наборов. Монотонность предметных наборов. Доминирование подсчета пар. Алгоритм Apriori. Применение Apriori для поиска всех частых предметных наборов. Обработка больших наборов данных в оперативной памяти. Алгоритм Парка-Чена-Ю (PCY). Многоэтапный алгоритм. Многохэшевый алгоритм. Алгоритм с ограниченным числом проходов. Простой рандомизированный алгоритм. Предотвращение ошибок в алгоритмах формирования выборки. Алгоритм SON. Алгоритм SON и MapReduce. Алгоритм Тойвонена. Подсчет частых предметных наборов в потоке. Методы выборки из потока. Частые предметные наборы в затухающих окнах. Гибридные методы.


1. Постройте линейную регрессию по заданному набору данных.
2. Постройте логистическую регрессию по обучающему набору ([1, 2], +1), ([2, 3], −1), ([2, 1], +1), ([3, 2], −1).
3. Для обучающего набора ([1, 2], +1), ([2, 3], −1), ([2, 1], +1), ([3, 2], −1) найдите все векторы w и значения порога θ, для которых гиперплоскость w x − θ = 0 корректно разделяет точки набора.
4. Что будет, если предпринять попытку обучения перцептрона на наборе данных ([1, 2], −1), ([2, 1], +1), ([2, 3], +1), ([3, 2], −1) с нулевым порогом? Возможно ли изменить порог и получить перцептрон, корректно классифицирующий эти точки? Предложите преобразование с помощью квадратичных многочленов, преобразующее эти точки так, что они станут линейно разделимы.
5. Рассмотрим одномерный набор данных (1, 1), (2, 2), (4, 3), (8, 4), (16, 5), (32, 6). Найдите функцию f(q), возвращающую метку в ответ на запрос q со следующей интерполяцией:
(a) Метка ближайшего соседа.
(b) Среднее меток двух ближайших соседей.
(c) Среднее, взвешенное по расстоянию, двух ближайших соседей.
(d) (Невзвешенное) среднее трех ближайших соседей.

1. Предположим, что нам доступно n бит памяти, а наше множество S имеет m элементов. Вместо использования k хэш-функций мы может разделить n бит на k массивов, и хэшировать единожды в каждый массив. В зависимости от n, m и k, какова вероятность ложноположительного значения? Сравните эту вероятность с вероятностью при использовании k хэш-функций в один массив.
2. Предположим, что наш поток состоит из целых чисел 3, 1, 4, 1, 5, 9, 2, 6, 5. Все наши хэш-функции будут вида h(x) = ax + b mod 32 для некоторых a и b. При этом результат следует трактовать как 5-битное двоичное целое число. Определите длину хвоста для каждого элемента потока и получающуюся оценку количества различных элементов, если хэш-функция равна
(a) h(x) = 2x + 1 mod 32.
(b) h(x) = 3x + 7 mod 32.
(c) h(x) = 4x mod 32.
3. Вычислите второй и третий момент для потока 3, 1, 4, 1, 3, 4, 2, 1, 2.
4. Найдите все способы, которыми последовательность битов 1001011011101
можно разделить по урнам. 
5. Дан набор из двенадцати корзин. В каждой находится три из шести наименований, от 1 до 6: {1, 2, 3}, {1, 3, 5}, {3, 5, 6}, {2, 3, 4}, {2, 4, 6}, {1, 2, 4}, {3, 4, 5}, {1, 3, 4}, {2, 3, 5}, {4, 5, 6}, {2, 4, 5}, {3, 4, 6}. Предположим, что порог носителя равен 4. При первом проходе алгоритма PCY используется хэш-таблица с 11 урнами, и множество  {i, j} хэшируется в урну i × j mod 11.
(a) Вычислите носитель для каждого наименования и каждой пары наименований.
(b) Какие пары хэшируются в какие урны?
(c) Какие урны наиболее частые?
(d) Какие пары подсчитываются при втором проходе алгоритма PCY?


1. Линейная регрессия. Метод наименьших квадратов.
2. Гребневая регрессия. Метод лассо. Эластичная сеть.
3. Байесовская гребневая регрессия.
4. Логистическая регрессия.
5. Регрессия Тейла-Сена. Метод RANSAC. 
6. Подходы к машинному обучению. Архитектура машинного обучения.
7. Перцептроны. 
8. Параллельная реализация перцептронов.
9. Метод опорных векторов. 
10. Нахождение решений в методе опорных векторов с помощью градиентного спуска.
11. Стохастический градиентный спуск.
12. Параллельная реализация метода опорных векторов.
13. Обучение по ближайшим соседям. 
14. Ядерная регрессия.
15. Сравнение методов обучения. 
16. Потоковая модель данных.
17. Проблемы обработки потоков.
18. Выборка данных из потока. Получение репрезентативной выборки. Динамическое изменение размера выборки. 
19. Фильтрация потоков. Фильтр Блума. 
20. Подсчет различных элементов в потоке. Алгоритм Флажоле-Мартена. 
21. Оценивание моментов. Алгоритм Алона-Матиаса-Сегеди для вторых моментов. 
22. Обработка бесконечных потоков.
23. Подсчет единиц в окне. Алгоритм Датара-Гиониса-Индыка-Мотвани.
24. Модель корзины покупок.
25. Представление данных о корзинах покупок.
26. Доминирование подсчета пар в модели корзины покупок. Алгоритм Apriori. 
27. Алгоритм Парка-Чена-Ю (PCY). 
28. Алгоритм SON. 
29. Алгоритм Тойвонена.
30. Подсчет частых предметных наборов в потоке.
