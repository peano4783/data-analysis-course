\documentclass[landscape]{slides}
\usepackage[landscape]{geometry}
\usepackage[russian]{babel}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{color}
\begin{document}

\begin{normalsize} % small


\begin{slide}
\begin{center}
Кластеризация

Clustering

\url{http://mmds.org}

Mining of Massive Datasets

Jure Leskovec, Anand Rajaraman, Jeff Ullman
\end{center}
\end{slide}


\begin{slide}
\textbf{\textcolor{blue}{Данные большой размерности}}

\begin{itemize}
\item По облаку точек данных требуется понять его структуру
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Задача кластеризации}}

\begin{itemize}
\item Дан набор точек, с понятием расстояния между точками, сгруппировать точки в некоторое
число кластеров, таким образом, чтобы
  \begin{itemize}
  \item Члены кластера были близки/похожи друг на друга
  \item Члены разных кластеров были не похожи
  \end{itemize}
\item Как правило:
  \begin{itemize}
  \item Точки находятся в пространстве большой размерности
  \item Схожесть определяется с помощью меры расстояний
    \begin{itemize}
    \item евклидова, косинусная, Жаккара, редакционное расстояние, ...
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Почему кластеризация -- сложная задача?}}

\begin{itemize}
\item Кластеризация в двух измерениях выглядит просто
\item Кластеризация малых объемов данных выглядит просто
\item В большинстве случаев это так и есть
\item Многие приложения привлекают не 2, а 10 или 10000 измерений
\item Пространства высокой размерности выглядят по-другому:
Почти все пары точек расположины примерно на одном расстоянии
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Задача кластеризации: галактики}}

\begin{itemize}
\item Каталог 2 миллиардов <<небесных объектов>> представляет объекты по их излучению в 7 измерениях (частотных диапазонах)
\item Задача: Кластеризовать по схожим объектам, например, галактики, находящиеся рядом звезды, и т.д.
\item Sloan Digital Sky Survey
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Задача кластеризации: музыкальные компакт-диски}}

\begin{itemize}
\item Интуитивно: музыка делится на категории, и покупатели предпочитают несколько категорий
  \begin{itemize}
  \item Но что такое категории в действительности?
  \end{itemize}
\item Представим компакт-диск по множеству покупателей, которые его купили
\item Похожие компакт-диски имеют похожий набор покупателей
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Задача кластеризации: музыкальные компакт-диски}}

Пространство всех компакт-дисков:
\begin{itemize}
\item Представим себе пространство с одним измерением для каждого покупателя
  \begin{itemize}
  \item Значения в измерении могут быть только 0 или 1
  \item Компакт-диск -- это точка в этом пространстве $(x_1,x_2,\ldots,x_k)$, где $x_i=1$ ттогда $i$-й покупатель купил этот компакт-диск
  \end{itemize}
\item Для Amazon размерность составляет десятки миллионов
\item Задача: найти кластеры схожих компакт-дисков
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Задача кластеризации: документы}}

\begin{itemize}
\item Представим документ как вектор $(x_1,x_2,\ldots,x_k)$, в котором $x_i=1$ ттогда
$i$-е слово (по некоторому порядку) встречается в документе
  \begin{itemize}
  \item В действительности не имеет значения, что $k$ бесконечно; т.е. мы не ограничиваем набор слов
  \end{itemize}
\item Документы со схожими наборами слов могут быть об одной и той же теме
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Меры схожести: косинусная, Жаккара и евклидова}}

\begin{itemize}
\item Как и в случае компакт-дисков, у нас имеется выбор, когда мы представляем документы
в виде наборов слов или шинглов:
  \begin{itemize}
  \item Наборы как векторы: Мера схожести -- косинусное расстояние
  \item Наборы как множества: Мера схожести -- расстояние Жаккара
  \item Наборы как точки: Мера схожести -- евклидово расстояние
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Обзор: методы кластеризации}}

\begin{itemize}
\item Иерархические:
  \begin{itemize}
  \item Агломерационный (снизу вверх)
    \begin{itemize}
    \item Изначально каждая точка -- кластер
    \item Многократно объединяем два <<ближайших>> кластера в один
    \end{itemize}
  \item Разделяющий (сверху вниз)
    \begin{itemize}
    \item Начать с одного кластера из рекурсивно его разбивать
    \end{itemize}
  \end{itemize}
\item Присвоение точек
  \begin{itemize}
  \item Хранить набор кластеров
  \item Точки принадлежат <<ближайшему>> кластеру
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Иерархическая кластеризация}}

\begin{itemize}
\item Ключевая операция: Многократное объединение двух ближайших кластеров
\item Три важных вопроса:
  \begin{itemize}
  \item Как представить кластер, состоящий из более одной точки?
  \item Как определить <<близость>> кластеров?
  \item Когда прекращать объединение кластеров?
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Иерархическая кластеризация}}

\begin{itemize}
\item Ключевая операция: Многократное объединение двух ближайших кластеров
\item (1) Как представить кластер из множества точек?
  \begin{itemize}
  \item Ключевая проблема: При объединении кластеров, как нам представить <<расположение>> кажого кластера, чтобы определить, какая пара кластеров наиболее близка?
  \end{itemize}
\item Евклидов случай: у каждого кластера есть \\
центроида = среднее его точек
\item (2) Как определить <<близость>> кластеров?
  \begin{itemize}
  \item Измерять расстояние между кластерами по расстоянию между центроидами
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{А что в неевклидовом случае?}}

А что в неевклидовом случае?

\begin{itemize}
\item Единственные <<расположения>>, о которых мы можем говорить, -- это сами точки
  \begin{itemize}
  \item т.е., нет никакого <<среднего>> между двумя точками
  \end{itemize}
\item Подход 1:
  \begin{itemize}
  \item Как представить кластер множества точек?\\
  Кластроида = точка, <<ближайшая>> к другим точкам
  \item Как определить <<близость>> кластеров?
  Относится к кластроиде так, как если бы это была центроида,
  при вычислении расстояний между кластерами
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{<<Ближайшая>> точка?}}

\begin{itemize}
\item Как представить кластер множества точек?\\
  Кластроида = точка, <<ближайшая>> к другим точкам
\item Возможные значения слова <<ближайшая>>:
  \begin{itemize}
  \item Наименьшее максимальное расстояние до других точек
  \item Наименьшее среднее расстояние до других точек
  \item Наименьшая сумма квадратов расстояний до других точек
    \begin{itemize}
    \item Для метрики расстояния $d$ кластроида $c$ кластера $C$ -- это
    $ \min_c \sum_{x\in C} d(x,c)^2$
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Определение <<близости>> кластеров}}

\begin{itemize}
\item (2) Как определить <<близость>> кластеров?
  \begin{itemize}
  \item Подход 2:\\
  Межкластерное расстояние = минимум расстояний между любыми двумя точками, по одной из каждого класса
  \item Подход 3:\\
  Введем понятие <<связности>> (cohersion) кластеров, например,
  максимальное расстояние от кластроиды
    \begin{itemize}
    \item Объединять кластеры, объединение которых получается наиболее связным
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Связность (Cohersion)}}

\begin{itemize}
\item Подход 3.1: Использовать диаметр объединяемого кластера =
максимальное расстояние между точками кластера
\item Подход 3.2: Использовать среднее расстояние между точками кластера
\item Подход 3.3: Использовать подход, основанный на плотности
  \begin{itemize}
  \item Взять диаметр среднего расстояния, например, и разделить на число точек в кластере
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Реализация}}

\begin{itemize}
\item Наивная реализация иерархической кластеризации:
  \begin{itemize}
  \item На каждом шаге вычислить попарные расстояния между всеми парами кластеров, затем выполнить объединение
  \item $O(N^3)$
  \end{itemize}
\item Тщательная реализация с ипользованием приоритетную очередь (priority queue)
  позволяет уменьшить время до $O(N^2 \log N)$
  \begin{itemize}
  \item Все равно слишком затратно для по-настоящему больших наборов данных, не помещающихся в памяти
  \end{itemize}
\end{itemize}
\end{slide}


\begin{slide}
Метод $k$-средних

($k$-means clustering)
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Алгоритм(ы) $k$-средних}}

\begin{itemize}
\item Предполагает евклидово пространство/расстояния
\item Начать с выбора $k$, числа кластеров
\item Инициализировать кластеры путем выбора одной точки на кластер
  \begin{itemize}
  \item Например: Выбрать случайно одну точку, затем $k-1$ других точек, каждую как можно дальше от предыдущих точек
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Наполнение кластеров}}

\begin{itemize}
\item 1) Для каждой точки, поместить ее в тот кластер, центроида которого ближе всего
\item 2) После того, как все точки распределены, обновить расположения центроид $k$ кластеров
\item 3) Перераспределить все точки к их ближайшим центроидам
  \begin{itemize}
  \item Иногда перемещает точки между кластерами
  \end{itemize}
\item Повторять 2) и 3) до сходимости
  \begin{itemize}
  \item Сходимость: точки не <<бегают>> между кластерами и центроиды стабилизируются
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Выбор правильного $k$}}

Как выбрать $k$?
\begin{itemize}
\item Пробовать различные $k$, наблюдая за изменением среднего расстояния до центроиды при увеличении $k$
\item Среднее быстро убывает до подходящего $k$, затем изменяется мало
\end{itemize}
\end{slide}


\begin{slide}
Алгоритм BFR

Обобщение $k$-средних на большие данные
\end{slide}


\begin{slide}
\textbf{\textcolor{blue}{Алгоритм BFR}}

\begin{itemize}
\item BFR [Bradley-Fayyad-Reina] -- это вариант метода $k$-средних,
предназначенный для обработки очень больших (хранящихся на диске) наборов данных
\item Предполагает, что кластеры нормально распределены вокруг центроиды в евклидовом пространстве
  \begin{itemize}
  \item Стандартные отклонения могут быть разными в разных координатах 
    \begin{itemize}
    \item Кластеры -- эллипсы, ориентированный по осям
    \end{itemize}
  \end{itemize}
\item Эффективный способ подытожить кластеры\\
(требуемая память -- O(кластеров), а не O(данных))
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Алгоритм BFR}}

\begin{itemize}
\item Points are read from disk one main-memory-full at a time
\item Большинство точек из предыдущих загрузок памяти подытоживаются простыми статистиками
\item Для начала, из первоначальной загрузыки выберем начальные $k$ центроиды некоторым sensible образом:
  \begin{itemize}
  \item Возьмем $k$ случайных точек
  \item Возьмем малую случайную выборку и кластер оптимальным образом
  \item Возьмем выборку; выберем случайную точку, а затем еще $k-1$ точек, каждую -- настолько далеко от выбранных точек, насколько возможно
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Три класса точек}}

Три набора точек, за которыми мы следим
\begin{itemize}
\item Discard set (DS):
  \begin{itemize}
  \item Точки, достаточно близкие к центроиде, чтобы быть подытоженными
  \end{itemize}
\item Compression set (CS):
  \begin{itemize}
  \item Группы точек, которые достаточно близки друг к другу, но не близки ни к какой существующей центроиде
  \item Эти точки подытоживаются, но не относятся ни к какому кластеру.
  \end{itemize}
\item Retained set (RS):
  \begin{itemize}
  \item Изолированные точки, ожидающие того, чтобы быть отнесенными к compression set
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Подытоживание набора точек}}
Для каждого кластера DS подытоживается по:

\begin{itemize}
\item Количество точек $N$
\item Вектор $SUM$, $i$-я компонента которого -- это сумма координат точек в $i$-м измерении
\item Вектор $SUMQ$: $i$-я компонента = сумма квадратов координат в $i$-м измерении
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Подытоживание точек: комментарии}}

\begin{itemize}
\item $2d+1$ значений представляют кластер любого размера
  \begin{itemize}
  \item $d$ = количество измерений
  \end{itemize}
\item Среднее в каждом измерении (центроида) может быть вычислена как $SUM_i/N$
  \begin{itemize}
  \item $SUM_i$ = $i$-я компонента вектора $SUM$
  \end{itemize}
\item Дисперсия множества DS кластера в размерности $i$ -- это:
  $(SUMSQ_i/N) - (SUM_i/N)^2$
  \begin{itemize}
  \item А стандартное отклонение -- это квадратный корень из этого значения
  \end{itemize}
\item Следующий шаг: Собственно кластеризация
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{<<Загрузка в память>> точек}}

Обработка <<загрузки в память>> точек (1):
\begin{itemize}
\item 1) Найти те точки, которые <<достаточно близки>> к центроиде кластера и добавить эти точки в этот кластер и DS
  \begin{itemize}
  \item Эти точки настолько близки к центроиде, что их можно подытожить и затем отбросить
  \end{itemize}
\item 2) Использовать любой алгоритм кластеризации, выполняющийся в оперативной памяти, для того, чтобы разбить на кластеры оставшиеся точки и старый RS
  \begin{itemize}
  \item Кластеры идут в CS; точки-выбросы идут в RS
  \end{itemize}
\end{itemize}
\end{slide}


\begin{slide}
\textbf{\textcolor{blue}{<<Загрузка в память>> точек}}

Обработка <<загрузки в память>> точек (2):
\begin{itemize}
\item 3) Множество DS: <<Подкрутить>> статистики кластеров, чтобы учесть новые точки
  \begin{itemize}
  \item Добавить $N$'ы, $SUM$'ы, $SUMSQ$'ы
  \end{itemize}
\item 4) Рассмотреть возможность объдинения сжатых множеств в CS
\item 5) Если это последний проход, объединить все сжатые множества в CS и все точки RS в их ближайший кластер
\end{itemize}
\end{slide}




\begin{slide}
\textbf{\textcolor{blue}{Немного подробностей...}}

\begin{itemize}
\item Q1) Как понять, что точка <<достаточно близка>> к кластеру настолько, что мы должны ее добавить в кластер?
\item Q2) Как понять, что два сжатых множества (CS) могут быть объединены в одно?
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Насколько близко <<достаточно близко>>?}}

\begin{itemize}
\item Q1) Нам нужен метод принятия решения о том, чтобы добавить новую точку в кластер (и выбросить ее)
\item BFR предлагает два метода:
  \begin{itemize}
  \item Расстояние Махаланобиса меньше, чем порог
  \item  Большое правдоподобие того, что точка принадлежит к ближайшей, в настоящее время, центроиде
  \end{itemize}
\end{itemize}
\end{slide}


\begin{slide}
\textbf{\textcolor{blue}{Расстояние Махаланобиса}}

\begin{itemize}
\item Нормированное Евклидово расстояние от центроиды
\item Для точки $(x_1,\ldots,x_d)$ и центроиды $(c_1,\ldots,c_d)$
  \begin{enumerate}
  \item Нормировать в каждом измерении: $y_i = (x_i-c_i)/\sigma_i$
  \item Взять сумму квадратов $y_i$
  \item Взять квадратный корень
  $$ d(x,c) = \sqrt{\sum_{i=1}^d \left(\frac{x_i-c_i}{\sigma_i}\right)^2} $$
  $\sigma_i$= стандартное отклонение точек в кластере по $i$-й размерности
  \end{enumerate}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Расстояние Махаланобиса}}

\begin{itemize}
\item Если кластеры нормально распределены в $d$ измерениях, то после преобразований одно стандартное отклонение = $\sqrt{d}$
  \begin{itemize}
  \item т.е., у 68\% точек кластера расстояние Махаланобиса $<\sqrt{d}$
  \end{itemize}
\item Принять точек в кластер, если ее расстояние Махаланобиса $<$ некоторого порога,
например, 2 стандартных отклонения
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Следует ли объединять 2 CS-кластера?}}

Q2) Следует ли объединять 2 CS-подкластера?
\begin{itemize}
\item Вычислить дисперсию объединенного подкластера
  \begin{itemize}
  \item $N$, $SUM$, $SUMSQ$ позволяют выполнить это вычисление быстро
  \end{itemize}
\item Объединить, если объединенная дисперсия ниже некоторого порога
\item Множество альтернатив: По-разному обращаться с измерениями, учитывать плотность
\end{itemize}
\end{slide}


\begin{slide}
Алгоритм CURE

Обобщение метода $k$-средних на кластеры произвольной формы
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Алгоритм CURE}}

\begin{itemize}
\item Проблемы с BFR/$k$-means:
  \begin{itemize}
  \item Предположение, что кластеры нормально распределены в каждом измерении
  \item И оси фиксированы -- эллипсы под углом НЕ допускаются
  \end{itemize}
\item CURE (Clustering Using REpresentatives):
  \begin{itemize}
  \item Предполагает евклидово расстояние
  \item Позволяет кластерам принимать любую форму
  \item Использует набор точек-представителей для представления кластеров
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Начало CURE}}

Алгоритм в два прохода. Проход 1:

\begin{itemize}
\item 0) Выбрать случайную выборку точек, умещающуюся в оперативной памяти
\item 1) Начальные кластеры:
  \begin{itemize}
  \item Разбить эти точки на кластеры иерархически -- группируем ближайшие точки/кластеры
  \end{itemize}
\item 2) Выбрать точки-представители
  \begin{itemize}
  \item Для каждого кластера, произвести выборку точек, настолько разбросанных, насколько возможно
  \item Из выборки выбрать представителей посредством их перемещения (например) на 20\% ближе к центроиде кластера
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Окончание CURE}}

Проход 2:

\begin{itemize}
\item Теперь перепросмотреть весь набор данных и перебрать
каждую точку $p$ в наборе данных
\item Поместить ее в <<ближайший кластер>>
  \begin{itemize}
  \item Нормальное определение <<ближайшего>>:\\
  Найти ближайшее представление к $p$ и приписать его к кластеру представителя
  \end{itemize}
\end{itemize}
\end{slide}



\begin{slide}
\textbf{\textcolor{blue}{Резюме}}

\begin{itemize}
\item Кластеризация: 
\item Алгоритмы:
  \begin{itemize}
  \item Агломерационная иерархическая кластеризация:
    \begin{itemize}
    \item Центроида и кластроида
    \end{itemize}
  \item $k$-means:
    \begin{itemize}
    \item Инициализация, выбор $k$
    \end{itemize}
  \item BFR
  \item CURE
  \end{itemize}
\end{itemize}
\end{slide}



\end{normalsize}

\end{document}